# SRPO (Self-Referential Policy Optimization) Code Implementation in siiRL

This document provides a comprehensive guide to understanding the SRPO algorithm implementation in siiRL. SRPO is designed for training Vision-Language-Action (VLA) models in embodied AI scenarios.

> **Paper Reference**: [SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models](https://arxiv.org/pdf/2511.15605)

## 1. Overview: What is SRPO?

**Self-Referential Policy Optimization (SRPO) for Vision-Language-Action Models** is a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging successful trajectories generated by the model within the current training batch as self-references. This enables us to assign progress-based rewards to failed attempts.

A core innovation is the use of **latent world representations** (V-JEPA) to robustly measure behavioral progress. Rather than relying on raw pixels or requiring domain-specific fine-tuning, we utilize compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, making trajectory comparison accurate and generalizable.

Empirical evaluation on the LIBERO benchmark demonstrates SRPO's efficiency and effectiveness. Starting from a supervised baseline with a 48.9% success rate, SRPO achieves a 99.2% success rate on novel states within only 200 RL steps, representing a 103% relative improvement without any additional supervision. Furthermore, SRPO shows significant robustness on the LIBERO-Plus benchmark, achieving a 167% performance gain.

**In siiRL, SRPO is implemented as the `embodied` workflow + `GRPO` advantage estimator.**

## 2. Code Architecture Overview

```
siiRL/
├── siirl/
│   ├── client/
│   │   ├── config/
│   │   │   ├── workflow_embodied_grpo.yaml    # DAG workflow definition
│   │   │   └── embodied_grpo_trainer.yaml     # Training configuration
│   │   └── main_dag.py                        # Entry point
│   ├── algorithm/
│   │   └── embodied/
│   │       └── sampling.py                    # Data filtering logic
│   ├── workers/
│   │   ├── dag_worker/
│   │   │   ├── core_algos.py                  # GRPO advantage & PPO loss
│   │   │   └── mixins/
│   │   │       └── node_executors_mixin.py    # Node execution logic
│   │   ├── rollout/
│   │   │   └── embodied_rollout.py            # VLA-Environment interaction loop
│   │   ├── environment/
│   │   │   └── embodied.py                    # LIBERO environment adapter
│   │   └── actor/
│   │       └── embodied_actor.py              # VLA model forward/backward
│   └── utils/
│       ├── reward_score/
│       │   └── embodied.py                    # VJEPA reward computation
│       └── embodied/
│           └── video_emb.py                   # V-JEPA embedding model
└── examples/
    └── embodied_grpo_trainer/
        └── run_openvla_oft_libero_*.sh        # Training scripts
```

## 3. Training Pipeline (DAG Workflow)

The SRPO training pipeline is defined in `workflow_embodied_grpo.yaml`:

```yaml
dag_id: "embodied_grpo_training_pipeline"
nodes:
  # Step 1: Environment rollout
  - node_id: "rollout_actor"
    node_type: "MODEL_INFERENCE"
    node_role: "ROLLOUT"
    
  # Step 2: Data filtering
  - node_id: "embodied_sampling"
    node_type: "COMPUTE"
    executable_ref: "siirl.algorithm.embodied.embodied_local_rank_sampling"
    
  # Step 3: Data rebalancing
  - node_id: "data_rebalance"
    node_type: "COMPUTE"
    node_role: "DATA_REBALANCE"
    
  # Step 4: VJEPA reward computation
  - node_id: "compute_reward"
    node_type: "COMPUTE"
    node_role: "REWARD"
    
  # Step 5: GRPO advantage calculation
  - node_id: "calculate_advantages"
    node_type: "COMPUTE"
    node_role: "ADVANTAGE"
    
  # Step 6: Compute old log probabilities
  - node_id: "actor_old_log_prob"
    node_type: "MODEL_TRAIN"
    node_role: "ACTOR"
    only_forward_compute: true
    
  # Step 7: Reference log probabilities (for KL penalty)
  - node_id: "reference_log_prob"
    node_type: "MODEL_TRAIN"
    node_role: "REFERENCE"
    
  # Step 8: Actor training
  - node_id: "actor_train"
    node_type: "MODEL_TRAIN"
    node_role: "ACTOR"
```

### Data Flow Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          SRPO Training Pipeline                              │
└─────────────────────────────────────────────────────────────────────────────┘

┌──────────────┐    ┌──────────────────┐    ┌─────────────────┐
│  DataLoader  │───▶│  rollout_actor   │───▶│embodied_sampling│
│  (task_id,   │    │ (VLA + Env)      │    │ (filter by acc) │
│   trial_id)  │    │                  │    │                 │
└──────────────┘    └──────────────────┘    └────────┬────────┘
                           │                         │
                           ▼                         ▼
                    ┌──────────────────┐    ┌─────────────────┐
                    │ Generated Data:  │    │  data_rebalance │
                    │ - input_ids      │    │ (cross-rank)    │
                    │ - pixel_values   │    └────────┬────────┘
                    │ - responses      │             │
                    │ - complete       │             ▼
                    │ - finish_step    │    ┌─────────────────┐
                    │ - vjepa_embedding│    │ compute_reward  │
                    └──────────────────┘    │ (VJEPA-based)   │
                                            └────────┬────────┘
                                                     │
    ┌────────────────────────────────────────────────┘
    │
    ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│calculate_advant │───▶│actor_old_log_   │───▶│reference_log_   │
│ages (GRPO)      │    │prob             │    │prob             │
└─────────────────┘    └─────────────────┘    └────────┬────────┘
                                                       │
                                                       ▼
                                              ┌─────────────────┐
                                              │  actor_train    │
                                              │ (PPO Loss)      │
                                              └─────────────────┘
```

## 4. Core Components Deep Dive

### 4.1 Rollout: Environment Interaction

**File**: `siirl/workers/dag_worker/mixins/node_executors_mixin.py`

**Function**: `generate_embodied_mode()`

The rollout stage generates trajectories by having the VLA model interact with the simulation environment (e.g., LIBERO).

```python
def generate_embodied_mode(self, worker_group_index: int, batch: DataProto, **kwargs) -> NodeOutput:
    """
    Generates embodied episodes for training.
    
    Input batch contains task metadata (task_id, trial_id) from the dataloader.
    The rollout worker generates all required data during environment interaction:
    - input_ids, attention_mask (tokenized observations)
    - pixel_values (visual observations)
    - responses (action tokens)
    - complete (success flag)
    - finish_step (episode length)
    - vjepa_embedding (visual features for reward)
    """
    rollout_worker = self.agent_group_worker[worker_group_index][NodeRole.ROLLOUT]
    
    # Set meta_info for embodied training
    batch.meta_info = {
        "n_samples": self.config.actor_rollout_ref.rollout.n,  # Group size (e.g., 8)
        ...
    }
    
    # Generate N trajectories per prompt
    gen_output = rollout_worker.generate_sequences(batch)
    
    # Expand original batch to match N samples and union with generated data
    batch = batch.repeat(self.config.actor_rollout_ref.rollout.n, interleave=True).union(gen_output)
    
    return NodeOutput(batch=batch, metrics=metrics)
```

**Key Output Fields**:
| Field | Shape | Description |
|-------|-------|-------------|
| `responses` | `(B*N, traj_len, action_dim)` | Action tokens (e.g., 7-dim: xyz + quat + gripper) |
| `complete` | `(B*N,)` | Boolean: task success flag |
| `finish_step` | `(B*N,)` | Integer: episode termination step |
| `vjepa_embedding` | `(B*N, embed_dim)` | V-JEPA visual features |

---

### 4.2 Embodied Rollout: VLA-Environment Interaction Loop

**File**: `siirl/workers/rollout/embodied_rollout.py`

**Class**: `EmbodiedHFRollout`

This is the core component that orchestrates the interaction between the VLA model and the simulation environment. It handles the complete episode generation process including action prediction, environment stepping, and visual embedding extraction.

#### 4.2.1 Class Initialization

```python
class EmbodiedHFRollout(BaseRollout):
    def __init__(self, module: nn.Module, config: ActorRolloutRefArguments):
        self.model = module  # VLA model (e.g., OpenVLA-OFT)
        
        # Initialize V-JEPA embedding model for reward computation
        self.embedding_model = VideoEmbeddingModel(
            model_path=config.embodied.video_embedding_model_path,
            img_size=config.embodied.embedding_img_size,
            enable_fp16=config.embodied.embedding_enable_fp16
        )
        
        # Initialize LIBERO environment adapter with parallel environments
        self.num_workers = config.embodied.env.num_envs  # e.g., 16 parallel envs
        self.adapter = LIBEROAdapter(
            env_name=config.embodied.env.env_name,      # e.g., "libero_goal"
            num_envs=self.num_workers,
            max_steps=config.embodied.env.max_steps,    # e.g., 512
            num_steps_wait=config.embodied.env.num_steps_wait,
            model_family=config.embodied.env.model_family,
            gpu_ids=[self._rank % self._num_gpus_per_node]
        )
```

#### 4.2.2 Main Entry Point: `generate_sequences()`

```python
def generate_sequences(self, prompts):
    """
    Main entry point for generating sequences.
    Splits large batches into chunks that fit the number of parallel workers.
    """
    total_batch_size = prompts.batch.batch_size[0]
    n_samples = prompts.meta_info.get('n_samples', 1)  # Group size for GRPO
    
    # Each prompt needs n_samples trajectories
    # batch_size_per_chunk = num_workers // n_samples
    batch_size_per_chunk = self.num_workers // n_samples
    num_chunks = (total_batch_size + batch_size_per_chunk - 1) // batch_size_per_chunk
    
    all_chunk_outputs = []
    for i in range(num_chunks):
        chunk_prompts = prompts[start_idx:end_idx]
        chunk_output = self._generate_chunk_rollout(chunk_prompts)
        all_chunk_outputs.append(chunk_output)
    
    return DataProto.concat(all_chunk_outputs)
```

#### 4.2.3 Episode Generation Loop: `_generate_chunk_rollout()`

This is the heart of the embodied rollout - a step-by-step interaction loop between the VLA model and the environment.

```python
def _generate_chunk_rollout(self, prompts):
    """
    Generate complete episodes for a chunk of tasks.
    """
    n_samples = prompts.meta_info.get('n_samples', 1)
    chunk_size = task_id.size(0)  # = batch_size * n_samples
    max_steps = self.config.embodied.env.max_steps
    
    # ═══════════════════════════════════════════════════════════════
    # Step 1: Reset all parallel environments
    # ═══════════════════════════════════════════════════════════════
    init_data_list = self.adapter._blocking_reset(
        task_ids=task_id.reshape(-1).cpu().numpy().tolist(),
        trial_ids=trial_id.reshape(-1).cpu().numpy().tolist(),
    )
    
    # Collect initial observations
    inputs = [self._obs_to_input(init_data['obs']) for init_data in init_data_list]
    task_descriptions = [init_data["task_description"] for init_data in init_data_list]
    task_records = [{
        "active": init_data['active'],
        "complete": init_data['complete'],
        "finish_step": init_data['finish_step'],
        "task_file_name": init_data['task_file_name']
    } for init_data in init_data_list]
    
    # ═══════════════════════════════════════════════════════════════
    # Step 2: Main interaction loop (up to max_steps)
    # ═══════════════════════════════════════════════════════════════
    step = 0
    vla_history = []  # Store all step data for training
    
    while step < max_steps:
        # Get indices of still-active environments
        active_indices = [i for i, r in enumerate(task_records) if r['active']]
        
        # ─────────────────────────────────────────────────────────────
        # Step 2a: Process observations into VLA input format
        # ─────────────────────────────────────────────────────────────
        vla_input = self.process_input(inputs, task_descriptions)
        # vla_input contains: input_ids, attention_mask, pixel_values
        
        # ─────────────────────────────────────────────────────────────
        # Step 2b: VLA model predicts actions
        # ─────────────────────────────────────────────────────────────
        vla_output = self._generate_one_step(vla_input)
        actions = vla_output["action"]  # (chunk_size, action_dim)
        
        # Store step data for later training
        vla_history.append({
            "responses": vla_output["responses"],    # Action tokens
            "input_ids": vla_output["input_ids"],
            "attention_mask": vla_output["attention_mask"],
            "pixel_values": vla_output["pixel_values"],
            "action": actions,
            "step": step
        })
        
        # ─────────────────────────────────────────────────────────────
        # Step 2c: Execute actions in environment
        # ─────────────────────────────────────────────────────────────
        step_results_list = self.adapter._blocking_step({
            "indices": active_indices,
            "actions": actions,
        })
        
        # ─────────────────────────────────────────────────────────────
        # Step 2d: Update observations and task status
        # ─────────────────────────────────────────────────────────────
        for idx in active_indices:
            result = step_results_list[idx]
            inputs[idx] = self._obs_to_input(result['obs'])
            task_records[idx]['active'] = result['active']      # Still running?
            task_records[idx]['complete'] = result['complete']  # Task succeeded?
            task_records[idx]['finish_step'] = result['finish_step']
            all_video[task_records[idx]['task_file_name']].extend(result['valid_images'])
        
        step += self.config.embodied.action_chunks_len  # e.g., += 8
    
    # ═══════════════════════════════════════════════════════════════
    # Step 3: Post-processing - Stack history and compute embeddings
    # ═══════════════════════════════════════════════════════════════
    batch = {}
    for k in ["responses", "input_ids", "attention_mask", "pixel_values"]:
        batch[k] = torch.stack([h[k] for h in vla_history], dim=1)
    
    batch["complete"] = torch.tensor([r["complete"] for r in task_records])
    batch["finish_step"] = torch.tensor([r["finish_step"] for r in task_records])
    
    # ═══════════════════════════════════════════════════════════════
    # Step 4: Extract V-JEPA embeddings for reward computation
    # ═══════════════════════════════════════════════════════════════
    vjepa_embeddings = self.embedding_model.get_embeddings(
        batch_names=[r['task_file_name'] for r in task_records],
        batch_frames=[all_video[r['task_file_name']] for r in task_records]
    )
    batch["vjepa_embedding"] = torch.tensor(np.array(vjepa_embeddings))
    
    return DataProto(batch=TensorDict(batch, batch_size=chunk_size))
```

#### 4.2.4 Single-Step Action Generation: `_generate_one_step()`

```python
@torch.no_grad()
def _generate_one_step(self, prompts: dict):
    """
    Generate one action chunk from VLA model.
    Supports both OpenVLA and OpenVLA-OFT model architectures.
    """
    if self.config.embodied.embodied_type == "openvla-oft":
        # OpenVLA-OFT: Action Flow Transformer variant
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            actions, response = self.model.generate_action_verl(
                input_ids=idx,
                pixel_values=pixel_values,
                attention_mask=attention_mask,
                do_sample=do_sample,
                unnorm_key=self.config.embodied.unnorm_key,
                temperature=temperature,
            )
        # response shape: (batch_size, action_chunks_len * action_token_len)
        # e.g., (16, 8 * 7) = (16, 56) for 8 action chunks with 7-dim actions
        
    elif self.config.embodied.embodied_type == "openvla":
        # Standard OpenVLA: Autoregressive token generation
        output = self.model.generate(
            input_ids=idx,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            do_sample=do_sample,
            max_new_tokens=response_length,
            temperature=temperature,
        )
        # Decode action tokens to continuous actions
        predicted_action_token_ids = output.sequences[:, prompt_length:]
        discretized_actions = self.model.vocab_size - predicted_action_token_ids
        normalized_actions = self.model.bin_centers[discretized_actions]
        # Unnormalize using task-specific statistics
        actions = unnormalize_actions(normalized_actions, self.config.embodied.unnorm_key)
    
    return {
        'responses': response,      # Action tokens for training
        'input_ids': idx,
        'attention_mask': attention_mask,
        'pixel_values': pixel_values,
        'action': actions,          # Continuous actions for environment
    }
```

#### 4.2.5 Observation Processing: `_obs_to_input()`

```python
def _obs_to_input(self, obs):
    """
    Convert raw environment observation to VLA input format.
    """
    from siirl.utils.embodied.libero_utils import get_libero_image, get_libero_wrist_image
    
    if self.config.embodied.num_images_in_input > 1:
        # Multi-view: primary + wrist camera
        return {
            "full_image": get_libero_image(obs, 224),
            "wrist_image": get_libero_wrist_image(obs, 224),
            "state": np.concatenate([
                obs["robot0_eef_pos"],           # End-effector position (3D)
                quat2axisangle(obs["robot0_eef_quat"]),  # Orientation (3D)
                obs["robot0_gripper_qpos"]       # Gripper state (1D)
            ])
        }
    else:
        # Single view: primary camera only
        return {
            "full_image": get_libero_image(obs, 224),
            "state": np.concatenate([...])
        }
```

#### 4.2.6 Embodied Rollout Data Flow Diagram

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        EmbodiedHFRollout._generate_chunk_rollout()               │
└─────────────────────────────────────────────────────────────────────────────────┘

┌──────────────┐                                                    ┌──────────────┐
│   Prompts    │                                                    │   Output     │
│  (task_id,   │                                                    │  DataProto   │
│   trial_id)  │                                                    │              │
└──────┬───────┘                                                    └──────▲───────┘
       │                                                                   │
       ▼                                                                   │
┌──────────────────────────────────────────────────────────────────────────┴───────┐
│                                                                                   │
│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐     │
│  │   LIBERO    │     │  process_   │     │  _generate_ │     │   LIBERO    │     │
│  │   reset()   │────▶│   input()   │────▶│  one_step() │────▶│   step()    │     │
│  │             │     │             │     │             │     │             │     │
│  │ Returns:    │     │ Creates:    │     │ VLA Model   │     │ Returns:    │     │
│  │ - obs       │     │ - input_ids │     │ Predicts:   │     │ - next_obs  │     │
│  │ - task_desc │     │ - attn_mask │     │ - actions   │     │ - complete  │     │
│  │             │     │ - pixel_val │     │ - responses │     │ - active    │     │
│  └─────────────┘     └─────────────┘     └─────────────┘     └──────┬──────┘     │
│        │                                                            │            │
│        │                    LOOP until max_steps                    │            │
│        └────────────────────────────────────────────────────────────┘            │
│                                                                                   │
│  ┌───────────────────────────────────────────────────────────────────────────┐   │
│  │                     Post-processing after loop                             │   │
│  │                                                                            │   │
│  │   vla_history ──▶ Stack tensors ──▶ batch["responses"], batch["input_ids"] │   │
│  │   task_records ──▶ batch["complete"], batch["finish_step"]                 │   │
│  │   all_video ──▶ VideoEmbeddingModel ──▶ batch["vjepa_embedding"]           │   │
│  │                                                                            │   │
│  └───────────────────────────────────────────────────────────────────────────┘   │
│                                                                                   │
└───────────────────────────────────────────────────────────────────────────────────┘
```

#### 4.2.7 Key Configuration Parameters for Rollout

| Parameter | Config Path | Description |
|-----------|-------------|-------------|
| `num_envs` | `actor_rollout_ref.embodied.env.num_envs` | Number of parallel environments (default: 16) |
| `max_steps` | `actor_rollout_ref.embodied.env.max_steps` | Maximum steps per episode (default: 512) |
| `action_chunks_len` | `actor_rollout_ref.embodied.action_chunks_len` | Actions per VLA forward pass (default: 8) |
| `action_token_len` | `actor_rollout_ref.embodied.action_token_len` | Action dimensions (default: 7 for xyz+quat+gripper) |
| `temperature` | `actor_rollout_ref.rollout.temperature` | Sampling temperature for action generation |
| `do_sample` | `actor_rollout_ref.rollout.do_sample` | Enable stochastic sampling (True for training) |

---

### 4.3 Data Filtering (Embodied Sampling)

**File**: `siirl/algorithm/embodied/sampling.py`

**Function**: `embodied_local_rank_sampling()` and `_filter_batch()`

This step filters out "too easy" or "too hard" prompts based on the success rate within each group.

```python
def _filter_batch(batch: DataProto, n_samples: int, siirl_args: SiiRLArguments) -> DataProto:
    """
    Filters a batch based on accuracy and truncation criteria.
    Filtering is performed at the prompt level.
    """
    # Reshape accuracy tensor: (B*N,) -> (B, N)
    acc_matrix = batch.batch["acc"].reshape(num_prompts, n_samples)
    
    # Calculate mean accuracy for each prompt
    prompt_mean_acc = acc_matrix.mean(dim=-1)
    
    # Filter: keep prompts where accuracy_lower_bound <= acc <= accuracy_upper_bound
    # Default: 0.1 <= acc <= 0.9 (exclude too easy/too hard)
    acc_mask = (prompt_mean_acc >= accuracy_lower_bound) & (prompt_mean_acc <= accuracy_upper_bound)
    
    # Expand prompt-level mask to sample-level
    final_mask = acc_mask.repeat_interleave(n_samples)
    
    return batch.select_idxs(final_mask)
```

**Why Filter?**
- **Too easy (acc > 0.9)**: All samples succeed → zero variance → zero advantage → no learning signal.
- **Too hard (acc < 0.1)**: All samples fail → similar issue.
- **Sweet spot (0.1 ≤ acc ≤ 0.9)**: Diverse outcomes → meaningful advantage estimates.

---

### 4.4 Reward Computation (VJEPA-based)

**File**: `siirl/utils/reward_score/embodied.py`

**Function**: `compute_embodied_reward()`

This is a key innovation of SRPO: using visual similarity to compute dense rewards for failed trajectories.

```python
def compute_embodied_reward(batch_data: DataProto, **kwargs) -> List[Dict[str, Any]]:
    """
    Computes rewards based on VJEPA embeddings and task completion status.
    
    Reward Formula:
    - Success: reward = 1.0
    - Failure: reward = sigmoid(distance_to_success_cluster) ∈ [0, 0.6]
    """
    # Extract data
    completes = batch_data.batch["complete"]       # (B,) bool
    embeddings = batch_data.batch["vjepa_embedding"]  # (B, embed_dim)
    
    # Initialize rewards
    final_rewards = np.zeros(batch_size)
    
    # Group by task
    for task_name, indices in task_to_valid_indices.items():
        success_indices = indices[completes[indices]]
        fail_indices = indices[~completes[indices]]
        
        # 1. Success trajectories get reward = 1.0
        final_rewards[success_indices] = 1.0
        
        if len(success_indices) == 0 or len(fail_indices) == 0:
            continue
        
        # 2. Cluster successful embeddings using DBSCAN
        succ_embeddings = embeddings[success_indices]
        clustering = DBSCAN(eps=0.5, min_samples=2).fit(succ_embeddings)
        cluster_centers = [succ_embeddings[clustering.labels_ == label].mean(axis=0) 
                          for label in set(clustering.labels_) if label != -1]
        
        # 3. Compute distance from failed trajectories to nearest success cluster
        fail_embeddings = embeddings[fail_indices]
        distance_matrix = cdist(fail_embeddings, cluster_centers, "euclidean")
        min_distances = distance_matrix.min(axis=1)
        
        # 4. Map distance to reward via sigmoid
        normalized_dists = (min_distances - min_dist) / (max_dist - min_dist)
        sigmoid_inputs = 10.0 * (0.5 - normalized_dists)
        reward_values = 0.6 * scipy.special.expit(sigmoid_inputs)
        
        final_rewards[fail_indices] = reward_values
    
    return results  # List of dicts with 'score' key
```

**Reward Visualization**:
```
Reward
  ^
1.0|  ●●● (Success)
   |
0.6|  ─────────────────────  (Max for failure)
   |      ╱
   |     ╱  Sigmoid curve
   |    ╱
0.0|───╱────────────────────▶ Distance to Success
       Near           Far
```

**Intuition**: Failed trajectories that are "visually similar" to successful ones (low distance) receive higher rewards, encouraging the policy to explore in promising directions.

---

### 4.5 Advantage Calculation (GRPO)

**File**: `siirl/workers/dag_worker/core_algos.py`

**Function**: `compute_grpo_outcome_advantage()`

GRPO computes advantages using group-relative normalization, eliminating the need for a Critic network.

```python
@register_adv_est(AdvantageEstimator.GRPO)
def compute_grpo_outcome_advantage(
    token_level_rewards: torch.Tensor,  # (B, response_length)
    response_mask: torch.Tensor,        # (B, response_length)
    index: np.ndarray,                  # (B,) - prompt index for grouping
    epsilon: float = 1e-6,
    norm_adv_by_std_in_grpo: bool = True,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    GRPO Advantage = (reward - group_mean) / group_std
    
    This is the "Self-Referential" part: the baseline is computed from
    the policy's own samples, not from a separate Value network.
    """
    # Sum rewards across response tokens to get scalar reward per sample
    scores = token_level_rewards.sum(dim=-1)  # (B,)
    
    # Group samples by prompt index
    id2score = defaultdict(list)
    for i in range(bsz):
        id2score[index[i]].append(scores[i])
    
    # Compute group statistics
    for idx in id2score:
        scores_tensor = torch.stack(id2score[idx])
        id2mean[idx] = torch.mean(scores_tensor)
        id2std[idx] = torch.std(scores_tensor)
    
    # Normalize: advantage = (score - mean) / std
    for i in range(bsz):
        if norm_adv_by_std_in_grpo:
            scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon)
        else:
            scores[i] = scores[i] - id2mean[index[i]]  # Dr.GRPO variant
    
    # Broadcast to token level
    advantages = scores.unsqueeze(-1) * response_mask
    
    return advantages, advantages  # (advantages, returns)
```

### 4.6 Policy Update (PPO Loss)

**File**: `siirl/workers/actor/embodied_actor.py`

**Function**: `update_policy()`

The actor update uses the standard PPO clipped objective with GRPO advantages.

```python
def update_policy(self, data: DataProto):
    """
    PPO policy update for VLA models.
    """
    self.actor_module.train()
    
    # Extract data
    old_log_prob = data['old_log_probs']   # π_old(a|s)
    advantages = data['advantages']         # GRPO advantages
    
    # Forward pass to get current log probs
    entropy, log_prob = self._forward_micro_batch_update(
        input_ids, attention_mask, pixel_values, responses, temperature
    )
    
    # Compute PPO clipped loss
    pg_loss, pg_clipfrac, ppo_kl, _ = core_algos.compute_policy_loss_vanilla(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        config=self.config
    )
    
    # Backward and optimize
    loss = pg_loss / self.gradient_accumulation
    loss.backward()
    
    grad_norm = self._optimizer_step()
```

**PPO Loss Function** (from `core_algos.py`):

```python
@register_policy_loss("vanilla")
def compute_policy_loss_vanilla(...):
    """
    L^CLIP(θ) = E[min(r_t(θ) * A_t, clip(r_t(θ), 1-ε, 1+ε) * A_t)]
    
    where r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)
    """
    # Importance ratio
    ratio = torch.exp(log_prob - old_log_prob)
    
    # Clipped objective
    pg_losses1 = -advantages * ratio
    pg_losses2 = -advantages * torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)
    pg_losses = torch.maximum(pg_losses1, pg_losses2)
    
    return pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower
```

---

## 5. Key Configuration Parameters

| Parameter | Location | Description |
|-----------|----------|-------------|
| `algorithm.adv_estimator` | `embodied_grpo_trainer.yaml` | Set to `grpo` for SRPO |
| `algorithm.workflow_type` | `embodied_grpo_trainer.yaml` | Set to `embodied` |
| `actor_rollout_ref.rollout.n` | Training script | Group size (samples per prompt) |
| `algorithm.embodied_sampling.filter_accuracy` | Training script | Enable accuracy-based filtering |
| `algorithm.embodied_sampling.accuracy_lower_bound` | Training script | Min success rate (default: 0.1) |
| `algorithm.embodied_sampling.accuracy_upper_bound` | Training script | Max success rate (default: 0.9) |
| `actor_rollout_ref.embodied.video_embedding_model_path` | Training script | Path to V-JEPA model |

## 6. Quick Reference: File Locations

| Component | File Path |
|-----------|-----------|
| Training Entry | `siirl/client/main_dag.py` |
| DAG Workflow | `siirl/client/config/workflow_embodied_grpo.yaml` |
| Rollout Dispatcher | `siirl/workers/dag_worker/mixins/node_executors_mixin.py` |
| **Embodied Rollout** | `siirl/workers/rollout/embodied_rollout.py` |
| Environment Adapter | `siirl/workers/environment/embodied.py` |
| V-JEPA Embedding | `siirl/utils/embodied/video_emb.py` |
| Data Filtering | `siirl/algorithm/embodied/sampling.py` |
| VJEPA Reward | `siirl/utils/reward_score/embodied.py` |
| GRPO Advantage | `siirl/workers/dag_worker/core_algos.py` |
| VLA Actor | `siirl/workers/actor/embodied_actor.py` |
| Example Scripts | `examples/embodied_grpo_trainer/run_openvla_oft_*.sh` |

## References

1. SRPO Paper: [Self-Referential Policy Optimization for Vision-Language-Action Models](https://arxiv.org/pdf/2511.15605)
2. V-JEPA: [V-JEPA: Latent Video Prediction for Visual Representation Learning](https://ai.meta.com/research/publications/v-jepa-latent-video-prediction-for-visual-representation-learning/)


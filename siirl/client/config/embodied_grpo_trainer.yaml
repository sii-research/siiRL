defaults:
  - ppo_dag_trainer
  - _self_

# Data configuration (simplified - removed duplicates)
data:
  dataset_type: embodied
  train_files: ~/data/libero/libero_10/train.parquet
  val_files: ~/data/libero/libero_10/test.parquet
  prompt_key: task_description
  video_key: video_frames
  max_prompt_length: 256          
  max_response_length: 128        
  train_batch_size: 64
  val_batch_size: 496

# Actor, Rollout, and Reference model configuration
actor_rollout_ref:
  model:
    path: ~/models/openvla-oft-7b
    enable_gradient_checkpointing: true
    trust_remote_code: true
    model_type: embodied
  
  actor:
    optim:
      lr: 5e-6                    
      weight_decay: 0.0
    ppo_mini_batch_size: 32       
    ppo_micro_batch_size_per_gpu: 1  
    ppo_epochs: 1
    grad_clip: 1.0
    shuffle: true                 # Shuffle training data
    
    clip_ratio_high: 0.28
    clip_ratio_low: 0.2
    clip_ratio_c: 10000.0         # Set very large to disable Dual-Clip PPO
    entropy_coeff: 0.0
    
    # FSDP configuration for OFT models
    fsdp_config:
      param_offload: false
      grad_offload: false
      optimizer_offload: false
    
    traj_mini_batch_size: 16
  
  rollout:
    # Use HF (HuggingFace) backend for FSDP-based rollout
    name: hf
    n: 8                          
    temperature: 1.6             
    do_sample: true
    response_length: 512
    log_prob_micro_batch_size_per_gpu: 1 
    micro_batch_size: 1           # Rollout inference micro batch size
    top_k: 0
    top_p: 1.0
  
  # Embodied AI specific configuration
  embodied:
    # Unified environment config for LIBERO
    env:
      env_type: libero
      env_name: libero_10         # REUSED: Task suite name
      num_envs: 16                # ALIGNED: 8 â†’ 16
      max_steps: 512              # Maximum steps per episode
      num_steps_wait: 10          # Environment stabilization steps
      num_trials_per_task: 50     # Number of trials per task
      model_family: openvla       # Model family for environment setup
    
    # Embodied model configuration
    embodied_type: openvla-oft
    video_embedding_model_path: ~/models/vjepa/vitg-384.pt
    
    # Action space configuration
    action_token_len: 7
    action_chunks_len: 8          # OFT uses action chunking
    
    # Image processing
    embedding_img_size: 384
    embedding_enable_fp16: true
    embedding_model_offload: false
    center_crop: true
    num_images_in_input: 1
    
    # Action normalization
    unnorm_key: libero_10_no_noops
      
  ref:
    log_prob_micro_batch_size_per_gpu: 1
    fsdp_config:
      param_offload: false

# Critic configuration - DISABLED for GRPO
critic:
  use_critic_model: false

# Reward model configuration for Embodied AI
reward_model:
  enable: false                       # No separate reward model in GRPO
  reward_manager: embodied            # Use EmbodiedRewardManager for validation
  reward_kwargs:                      # Embodied-specific reward kwargs
    action_token_len: 7               # Number of action tokens (must match model)
    reward_coef: 5.0                  # Reward coefficient for scaling

# Algorithm configuration - Standard GRPO with embodied sampling (aligned with DAPO architecture)
algorithm:
  workflow_type: embodied         # Uses workflow_embodied_grpo.yaml
  adv_estimator: grpo             # Standard GRPO for embodied tasks
  norm_adv_by_std_in_grpo: true   # Normalize advantages by std in GRPO
  gamma: 1.0
  lam: 1.0
  
  # Embodied sampling configuration (similar to DAPO's filter_groups)
  embodied_sampling:
    filter_accuracy: true         # Enable accuracy-based filtering
    accuracy_lower_bound: 0.1     # Min success rate to keep
    accuracy_upper_bound: 0.9     # Max success rate to keep
    filter_truncated: false       # Filter truncated episodes
    oversample_factor: 1          # Oversample factor for filtering

# Trainer configuration
trainer:
  logger:
    - console
    - tensorboard
  project_name: siirl_embodied_libero_10
  experiment_name: openvla_oft_grpo_fsdp
  n_gpus_per_node: 8
  nnodes: 1
  save_freq: 4                    
  test_freq: 4                   
  total_epochs: 1000
  resume_mode: auto
  max_actor_ckpt_to_keep: 5
  default_local_dir: ckpts/openvla_oft_grpo_fsdp_libero_10
  val_before_train: true
  val_only: false



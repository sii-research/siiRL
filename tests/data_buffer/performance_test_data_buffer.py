# Copyright 2025, Shanghai Innovation Institute. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio
import time
import ray
import torch
import numpy as np
from tensordict import TensorDict
from typing import List
import datetime
import random
import uuid

# æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„ï¼Œç¡®ä¿è¿™é‡Œçš„å¯¼å…¥è·¯å¾„æ˜¯æ­£ç¡®çš„
from siirl.data_coordinator.data_buffer import init_data_coordinator
from siirl.data_coordinator.sample import SampleInfo

# ====================================================================
# Performance Test Configuration for New Architecture
# ====================================================================
# --- Data Generation Parameters ---
# æ€»å…±è¦ç”Ÿæˆçš„æ ·æœ¬æ•°é‡ (ä¸æ—§æµ‹è¯•å¯¹é½: 200 items * 8 batch/item = 1600 samples)
TOTAL_SAMPLES = 1600
# æ¯ä¸ªæ ·æœ¬å†…éƒ¨çš„batch size (é€šå¸¸ä¸º1ï¼Œæ¨¡æ‹Ÿå•ä¸ªtrajectory)
BATCH_SIZE_PER_SAMPLE = 1
# åºåˆ—é•¿åº¦ 
SEQ_LEN = 1024
# ç‰¹å¾ç»´åº¦ 
EMBED_DIM = 1024

# --- Workload Parameters ---
# æ¨¡æ‹Ÿå¹¶å‘ç”Ÿäº§æ•°æ®çš„RolloutWorkeræ•°é‡
NUM_PRODUCERS = 8
# æ¨¡æ‹Ÿçš„Trainerä¸€æ¬¡è·å–çš„batch size
TRAINER_BATCH_SIZE = 128
# åˆ†å¸ƒå¼DataBufferçš„æ•°é‡ (åº”ç­‰äºPRODUCERSæ•°é‡ï¼Œæ¨¡æ‹Ÿæ¯ä¸ªworkeræœ‰è‡ªå·±çš„æœ¬åœ°buffer)
NUM_BUFFERS = NUM_PRODUCERS


def log_with_time(message: str):
    """Prints a message with a timestamp and flushes the output."""
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    print(f"[{now}] {message}", flush=True)


def create_mock_sample(item_idx: int) -> TensorDict:
    """åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æ ·æœ¬(TensorDict)ç”¨äºæµ‹è¯•"""
    tensor_data = {
        "input_ids": torch.randint(0, 32000, (BATCH_SIZE_PER_SAMPLE, SEQ_LEN)),
        "attention_mask": torch.ones(BATCH_SIZE_PER_SAMPLE, SEQ_LEN, dtype=torch.long),
        "hidden_states": torch.randn(BATCH_SIZE_PER_SAMPLE, SEQ_LEN, EMBED_DIM),
    }
    return TensorDict(tensor_data, batch_size=[BATCH_SIZE_PER_SAMPLE])


async def producer_task(
    producer_id: int, 
    coordinator: ray.actor.ActorHandle, 
    num_samples_to_produce: int
):
    """æ¨¡æ‹Ÿä¸€ä¸ªRolloutWorkerçš„è¡Œä¸ºï¼šç”Ÿäº§æ•°æ®ã€å­˜å…¥æœ¬åœ°ã€æ³¨å†Œåˆ°å…¨å±€ã€‚"""
    for i in range(num_samples_to_produce):
        sample_data = create_mock_sample(producer_id * num_samples_to_produce + i)
        
        # 1. å°†æ ·æœ¬æ•°æ®å­˜å…¥å½“å‰èŠ‚ç‚¹çš„Rayå¯¹è±¡å­˜å‚¨ä¸­
        sample_ref = ray.put(sample_data)
        
        # 2. åˆ›å»ºå…ƒæ•°æ®
        sample_info = SampleInfo(
            agent_group=producer_id,
            sum_tokens=SEQ_LEN,
            prompt_length=SEQ_LEN,
            response_length=0, # å‡è®¾åœ¨produceré˜¶æ®µresponseä¸ºç©º
            uid=uuid.uuid4().int # ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°ID
        )
        
        # 3. å°†å…ƒæ•°æ®å’Œå¼•ç”¨æ³¨å†Œåˆ°å…¨å±€DataCoordinator
        #    DataCoordinatorä¼šè‡ªåŠ¨å¤„ç†å¼•ç”¨çš„æœ¬åœ°æŒæœ‰
        await coordinator.put.remote(sample_info, sample_ref)


async def main():
    """ä¸»æ€§èƒ½æµ‹è¯•å‡½æ•°"""
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True, logging_level="error")

    log_with_time("=" * 80)
    log_with_time("      New DataCoordinator Architecture - Performance Benchmark")
    log_with_time("=" * 80)
    log_with_time(f"Configuration:")
    log_with_time(f"  - Total Samples to Generate: {TOTAL_SAMPLES}")
    log_with_time(f"  - Concurrent Producers (RolloutWorkers): {NUM_PRODUCERS}")
    log_with_time(f"  - Trainer Batch Size: {TRAINER_BATCH_SIZE}")
    log_with_time(f"  - Distributed Buffers: {NUM_BUFFERS}")
    log_with_time("-" * 80)

    # 1. åˆå§‹åŒ–æ•°æ®åè°ƒç³»ç»Ÿ
    # åœ¨å•æœºæµ‹è¯•æ—¶ï¼Œå¿…é¡»è®¾ç½® force_local=Trueï¼Œä»¥é¿å…ç­‰å¾…å¤šä¸ªèŠ‚ç‚¹
    coordinator = init_data_coordinator(NUM_BUFFERS, force_local=True)
    log_with_time(f"âœ… Data system initialized with 1 Coordinator.")

    # 2. æµ‹è¯•å¹¶å‘ Producer (RolloutWorker) æ€§èƒ½
    log_with_time("\n--- Testing Concurrent Producer (put) Performance ---")
    
    samples_per_producer = TOTAL_SAMPLES // NUM_PRODUCERS
    if TOTAL_SAMPLES % NUM_PRODUCERS != 0:
        log_with_time(f"Warning: Total samples not evenly divisible by producers. Some producers will generate more samples.")
    
    start_time = time.perf_counter()
    
    producer_tasks = []
    for i in range(NUM_PRODUCERS):
        # RolloutWorkerç°åœ¨åªéœ€è¦ä¸Coordinatoräº¤äº’
        num_to_produce = samples_per_producer + (1 if i < TOTAL_SAMPLES % NUM_PRODUCERS else 0)
        if num_to_produce > 0:
            task = producer_task(i, coordinator, num_to_produce)
            producer_tasks.append(task)
            
    await asyncio.gather(*producer_tasks)
    end_time = time.perf_counter()

    total_put_time = end_time - start_time
    put_throughput = TOTAL_SAMPLES / total_put_time
    avg_put_latency = (total_put_time / TOTAL_SAMPLES) * 1000  # ms per sample

    log_with_time(f"  - Total time to produce and register {TOTAL_SAMPLES} samples: {total_put_time:.4f} seconds")
    log_with_time(f"  - Producer Throughput: {put_throughput:.2f} samples/sec")
    log_with_time(f"  - Average Producer Latency: {avg_put_latency:.4f} ms/sample")
    
    # éªŒè¯æ•°æ®æ˜¯å¦å·²å…¨éƒ¨å­˜å…¥Coordinator
    queue_size = await coordinator.get_valid_size.remote()
    assert queue_size == TOTAL_SAMPLES, f"Coordinator size mismatch! Expected {TOTAL_SAMPLES}, got {queue_size}"
    log_with_time("âœ… All samples registered in Coordinator.")

    # 3. æµ‹è¯• Consumer (Trainer) æ€§èƒ½
    log_with_time("\n--- Testing Consumer (get_batch) Performance ---")
    num_batches_to_get = TOTAL_SAMPLES // TRAINER_BATCH_SIZE
    log_with_time(f"Simulating a trainer fetching {num_batches_to_get} batches of size {TRAINER_BATCH_SIZE}.")
    
    consumer_start_time = time.perf_counter()
    
    total_retrieved_samples = 0
    for _ in range(num_batches_to_get):
        # 1. ä»Coordinatorè·å–ä¸€æ‰¹æ ·æœ¬çš„å¼•ç”¨ (æˆ–å› Rayä¼˜åŒ–è€Œç›´æ¥è·å¾—å€¼)
        batch_refs_or_values = await coordinator.get_batch.remote(TRAINER_BATCH_SIZE)
        if not batch_refs_or_values:
            log_with_time("  - Coordinator returned empty batch, stopping consumer test.")
            break
            
        # 2. åŒºåˆ†è¿”å›çš„æ˜¯ObjectRefè¿˜æ˜¯å·²è§£æçš„å€¼ï¼Œåˆ†åˆ«å¤„ç†
        resolved_batch = []
        refs_to_get = []
        for item in batch_refs_or_values:
            if isinstance(item, ray.ObjectRef):
                refs_to_get.append(item)
            else:
                # Rayå¯èƒ½å› ä¸ºè°ƒç”¨æ–¹æ˜¯æ‰€æœ‰è€…è€Œç›´æ¥è¿”å›å€¼
                resolved_batch.append(item)
        
        # æ‰¹é‡è·å–æ‰€æœ‰éœ€è¦è§£æçš„ObjectRef
        if refs_to_get:
            loop = asyncio.get_running_loop()
            resolved_from_refs = await loop.run_in_executor(None, ray.get, refs_to_get)
            resolved_batch.extend(resolved_from_refs)

        actual_data_batch = resolved_batch
        total_retrieved_samples += len(actual_data_batch)

    consumer_end_time = time.perf_counter()

    total_get_time = consumer_end_time - consumer_start_time
    get_throughput = total_retrieved_samples / total_get_time
    avg_batch_latency = (total_get_time / num_batches_to_get) * 1000 # ms per batch

    log_with_time(f"  - Total time for consumer to fetch {total_retrieved_samples} samples: {total_get_time:.4f} seconds")
    log_with_time(f"  - Consumer Throughput: {get_throughput:.2f} samples/sec")
    log_with_time(f"  - Average Batch Latency: {avg_batch_latency:.4f} ms/batch")
    
    assert total_retrieved_samples == num_batches_to_get * TRAINER_BATCH_SIZE, "Mismatch in retrieved sample count!"
    log_with_time("âœ… Data integrity check passed for consumer results.")

    # 4. æ¸…ç†èµ„æº
    log_with_time("\n\nğŸ§¹ Cleaning up resources...")
    # æˆ‘ä»¬æ— æ³•ç›´æ¥è®¿é—®data_buffersï¼Œä½†å¯ä»¥é€šè¿‡coordinatoré—´æ¥ç®¡ç†æˆ–ç›´æ¥é€šè¿‡ray kill
    # ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªkill coordinatorï¼Œå› ä¸ºå®ƒæ˜¯æˆ‘ä»¬å”¯ä¸€æŒæœ‰çš„å¥æŸ„
    ray.kill(coordinator, no_restart=True)
    # Note: Placement group and detached actors might need manual cleanup if not managed properly.
    # For this test, shutting down ray is sufficient.
    ray.shutdown()
    log_with_time("=" * 80)
    log_with_time("               Benchmark Finished")
    log_with_time("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
